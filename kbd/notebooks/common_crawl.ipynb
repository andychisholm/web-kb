{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "sqlContext = pyspark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import random\n",
    "import ujson as json\n",
    "from itertools import izip\n",
    "from operator import add, itemgetter\n",
    "from collections import Counter, defaultdict\n",
    "from urlparse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.connection import Key\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "s3baseuri = \"s3n://\"\n",
    "\n",
    "def zip_sum(*x):\n",
    "  return [sum(i) for i in izip(*x)]\n",
    "def trim_link_protocol(s):\n",
    "    idx = s.find('://')\n",
    "    return s if idx == -1 else s[idx+3:]\n",
    "def get_timestamp():\n",
    "  return datetime.fromtimestamp(time()).strftime('%Y%m%d%H%M%S')\n",
    "def write_file_to_s3(localfile, s3_bucket, s3_filename):\n",
    "  conn = S3Connection(key, secret)\n",
    "  bucket = conn.get_bucket(s3_bucket)\n",
    "  if len(list(bucket.list(s3_filename))) == 0:\n",
    "    k = Key(bucket)\n",
    "    k.key = s3_filename\n",
    "    k.set_contents_from_filename(localfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mention_aligned_links(doc):\n",
    "  text = doc['full_text']\n",
    "  for m in doc['mentions']:\n",
    "    mention_start, mention_stop = m['start'], m['stop']\n",
    "    # filter mentions which occur outside of document full_text\n",
    "    if mention_start >= 0 and mention_stop > mention_start:\n",
    "      link_start = mention_stop+2\n",
    "      # naively detect whether this mention sits inside a markdown link anchor\n",
    "      if text[mention_start-1] == '[' and text[mention_stop:link_start] == '](':\n",
    "        link_stop = text.find(')', link_start)\n",
    "\n",
    "        if text[link_start:link_stop].startswith('http://'):\n",
    "            link_start += 7\n",
    "        elif text[link_start:link_stop].startswith('https://'):\n",
    "            link_start += 8\n",
    "\n",
    "        if link_stop != -1:\n",
    "          yield slice(link_start, link_stop), slice(mention_start,mention_stop)\n",
    "\n",
    "def get_links(doc):\n",
    "  for m in re.finditer('(?<!\\\\\\\\)\\[(([^]]|(\\\\\\\\]))+)(]\\(\\s*(http[s]?://)?)([^)]+)\\s*\\)', doc['full_text']):\n",
    "    parts = m.groups()\n",
    "    a, uri = parts[0], parts[5]\n",
    "    if uri and not a.startswith('www') and not a.startswith('http') and not 'secure.adnxs.com' in uri:\n",
    "      if 'digg.com' in uri:\n",
    "        continue # todo: add check for anchor diversity to filter this kidn of thing\n",
    "      mention_start = m.start() + 1\n",
    "      mention_stop = mention_start + len(parts[0])\n",
    "      link_start = mention_stop + len(parts[3])\n",
    "      link_stop = link_start + len(parts[5])\n",
    "      yield slice(link_start, link_stop), slice(mention_start, mention_stop)\n",
    "\n",
    "import base64\n",
    "import urlparse\n",
    "def resolve_hardcoded_redirects(l):\n",
    "  try:\n",
    "    if l.startswith('www.prweb.net'):\n",
    "      l = base64.b64decode(l[len('www.prweb.net/Redirect.aspx?id='):])\n",
    "    elif l.startswith('cts.businesswire.com/ct/') or l.startswith('ctt.marketwire.com/'):\n",
    "      l = urlparse.parse_qs(l)['url'][0]\n",
    "  except: pass\n",
    "  return trim_link_protocol(l)\n",
    "\n",
    "anchor_filters = set([\n",
    "  'facebook',\n",
    "  'twitter',\n",
    "  'zacks investment research',\n",
    "  'reuters',\n",
    "  'linkedin',\n",
    "  'marketbeat'\n",
    "])\n",
    "\n",
    "if False:\n",
    "  def get_link_labels(doc):\n",
    "    text = doc['full_text']\n",
    "    aligned_spans = set()\n",
    "    for l, a in get_mention_aligned_links(doc):\n",
    "      aligned_spans.add((l.start, l.stop))\n",
    "      uri = text[l]\n",
    "      if not 'search' in uri and not text[a].lower().strip() in anchor_filters:\n",
    "        yield (1.0, uri)\n",
    "    for l, a in get_links(doc):\n",
    "      if (l.start, l.stop) not in aligned_spans:\n",
    "        yield (0.0, text[l])\n",
    "\n",
    "def get_anchor_target_pairs(doc):\n",
    "  text = doc['full_text']\n",
    "  aligned_spans = set()\n",
    "  for l, a in get_mention_aligned_links(doc):\n",
    "    aligned_spans.add((l.start, l.stop))\n",
    "    yield (text[a], resolve_hardcoded_redirects(text[l]), True)\n",
    "  for l, a in get_links(doc):\n",
    "    if (l.start, l.stop) not in aligned_spans:\n",
    "      is_mention = False\n",
    "      if text[a].startswith('@') and not ' ' in text[a]:\n",
    "        is_mention = True # twitter NER = solved\n",
    "      yield (text[a], resolve_hardcoded_redirects(text[l]), is_mention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URI Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_uri(uri):\n",
    "  uri = uri.lower()\n",
    "  if uri.startswith('//'):\n",
    "    uri = uri[2:]\n",
    "  if uri.startswith('www.'):\n",
    "    uri = uri[4:]\n",
    "\n",
    "  # trim uri protocol\n",
    "  idx = uri.find('://')\n",
    "  uri = uri[idx+3:] if idx != -1 else uri\n",
    "\n",
    "  # convert 'blah.com/users.php?id=bob' into 'blah.com/users.php/id=bob'\n",
    "  uri = re.sub('([a-z]+)\\?', r\"\\1/\", uri)\n",
    "  \n",
    "  # convert 'blah.com/users#bob' into 'blah.com/users/bob'\n",
    "  uri = uri.replace('#', '/')\n",
    "\n",
    "  parts = uri.rstrip('/').split('/')\n",
    "  suffix = parts[-1].lower()\n",
    "  if len(parts) > 1 and suffix.startswith('index') or suffix.startswith('default'):\n",
    "    parts = parts[:-1]\n",
    "  if len(parts) > 1:\n",
    "    parts[-1] = '<eid>'\n",
    "  else:\n",
    "    parts.append('<nil>')\n",
    "  return '/'.join(parts)\n",
    "\n",
    "#normalize_uri('vanityfair.com/index.aspx?rofl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_uri_domain(uri):\n",
    "  return uri.split('/')[0]\n",
    "\n",
    "def get_uri_features(uri):\n",
    "  features = []\n",
    "\n",
    "  uri_parts = re.sub('[0-9]', 'N', uri).split('/')\n",
    "  dom = uri_parts[0]\n",
    "  uri_parts[0] = \"<domain>\"\n",
    "  features += list('/'.join(p) for p in izip(uri_parts, uri_parts[1:]))\n",
    "  features += [dom+':'+f for f in features]\n",
    "  features += uri_parts\n",
    "\n",
    "  dom_parts = dom.split('.')\n",
    "  if len(dom_parts) >= 3:\n",
    "    features.append('SD:' + '.'.join(dom_parts[:-2]))\n",
    "  return features\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes, LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, StringIndexer, CountVectorizer\n",
    "\n",
    "def balance_dataset(dataset, minor = 1.0, major = 0.0):\n",
    "  major_count = dataset.filter(dataset.label == major).count()\n",
    "  minor_count = dataset.filter(dataset.label == minor).count()\n",
    "  return dataset.filter(dataset.label == major)\\\n",
    "                .sample(withReplacement=False, fraction=minor_count/float(major_count))\\\n",
    "                .unionAll(dataset.filter(dataset.label == minor))\n",
    "\n",
    "def stats_at_p(r, p):\n",
    "  tp = 1.0 if (r['label'] == 1.0 and r['probability'][1] >= p) else 0.0\n",
    "  fp = 1.0 if (r['label'] == 0.0 and r['probability'][1] >= p) else 0.0\n",
    "  fn = 1.0 if (r['label'] == 1.0 and r['probability'][1] < p) else 0.0\n",
    "  return p, (tp, fp, fn)\n",
    "\n",
    "def evaluate(dataset, ps = None):\n",
    "  if ps == None:\n",
    "    ps = [0.5]\n",
    "  stats_by_p = dataset\\\n",
    "    .flatMap(lambda r: (stats_at_p(r, p) for p in ps))\\\n",
    "    .reduceByKey(lambda a, b: [x+y for x,y in zip(a, b)])\\\n",
    "    .filter(lambda (p, (tp, fp, fn)): (tp+fp) > 0 and (tp+fn) > 0)\\\n",
    "    .mapValues(lambda (tp, fp, fn): ((float(tp) / (tp+fp)), (float(tp) / (tp+fn))))\\\n",
    "    .mapValues(lambda (p, r): (p, r, 2 * (p*r/(p+r))))\\\n",
    "    .collect()\n",
    "  return stats_by_p\n",
    "    \n",
    "classifier = LogisticRegression(featuresCol=\"hashed_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REBUILD_CORPUS = False\n",
    "\n",
    "raw_corpus_path = s3baseuri + 'abbrevi8-rnd/kb/live/20160301/articles'\n",
    "link_corpus_path = s3baseuri + 'abbrevi8-rnd/web/links/seed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anchor_target_pairs = sc\\\n",
    "  .textFile(raw_corpus_path)\\\n",
    "  .map(json.loads)\\\n",
    "  .flatMap(get_anchor_target_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/context.py:538: UserWarning: load is deprecated. Use read.load() instead.\n",
      "  warnings.warn(\"load is deprecated. Use read.load() instead.\")\n"
     ]
    }
   ],
   "source": [
    "if REBUILD_CORPUS:\n",
    "  train, test = [\n",
    "    split.flatMap(lambda (prefix, instances): instances)\\\n",
    "         .map(lambda (uri, is_mention): (uri, 1.0 if is_mention else 0.0, get_uri_features(uri)))\\\n",
    "         .repartition(128)\\\n",
    "         .cache()\n",
    "    for split in\n",
    "      anchor_target_pairs\\\n",
    "          .map(lambda (anchor, target, is_mention): (normalize_uri(target), is_mention))\\\n",
    "          .groupByKey()\\\n",
    "          .filter(lambda (k,vs): len(vs) >= 10)\\\n",
    "          .mapValues(Counter)\\\n",
    "          .mapValues(lambda cs: cs[True] > cs[False])\\\n",
    "          .map(lambda (uri, is_mention): (get_uri_domain(uri), (uri, is_mention)))\\\n",
    "          .groupByKey()\\\n",
    "          .randomSplit([0.9, 0.1])\n",
    "  ]\n",
    "  sqlContext\\\n",
    "    .createDataFrame(train, ['uri','label','features'])\\\n",
    "    .write.mode('overwrite')\\\n",
    "    .save(link_corpus_path + 'train')\n",
    "  sqlContext\\\n",
    "    .createDataFrame(test, ['uri','label','features'])\\\n",
    "    .write.mode('overwrite')\\\n",
    "    .save(link_corpus_path + 'test')\n",
    "\n",
    "train = sqlContext.load(link_corpus_path + 'train')\n",
    "test = sqlContext.load(link_corpus_path + 'test')\n",
    "full = train.unionAll(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10429, 90243)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.filter(train['label']==1.0).count(), train.filter(train['label']==0.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashing_tf = HashingTF(inputCol=\"features\", outputCol=\"hashed_features\", numFeatures=500000)\n",
    "train = hashing_tf.transform(train)\n",
    "test = hashing_tf.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_model = classifier.fit(balance_dataset(train).repartition(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation @ Confidence >= 0.5\n",
      "Train P/R=(0.87, 0.97), F=0.919\n",
      "  Dev P/R=(0.84, 0.52), F=0.641\n"
     ]
    }
   ],
   "source": [
    "train_prs = evaluate(dev_model.transform(train), ps=[p/40. for p in xrange(1, 40)])\n",
    "dev_prs = evaluate(dev_model.transform(test), ps=[p/40. for p in xrange(1, 40)])\n",
    "#test_prs = evaluate(dev_model.transform(hashing_tf.transform(labeled_uris)), ps=[p/20. for p in xrange(1, 20)])\n",
    "\n",
    "print 'Evaluation @ Confidence >= 0.5'\n",
    "print 'Train P/R=(%.2f, %.2f), F=%.3f' % dict(train_prs)[0.5]\n",
    "print '  Dev P/R=(%.2f, %.2f), F=%.3f' % dict(dev_prs)[0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence @ Optimal Dev F1 >= 0.825\n",
      "Train P/R=(0.89, 0.96), F=0.926\n",
      "  Dev P/R=(0.88, 0.51), F=0.645\n"
     ]
    }
   ],
   "source": [
    "c, (p_c, r_c, f_c) = sorted(dev_prs, key=lambda (c, (p,r,f)): f, reverse=True)[0]\n",
    "\n",
    "print 'Confidence @ Optimal Dev F1 >= %.3f' % c\n",
    "print 'Train P/R=(%.2f, %.2f), F=%.3f' % dict(train_prs)[c]\n",
    "print '  Dev P/R=(%.2f, %.2f), F=%.3f' % dict(dev_prs)[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = classifier.fit(balance_dataset(hashing_tf.transform(full)).repartition(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'facebook.com/efoim', 0.46599315786468753),\n",
       " (u'twitter.com/person', 0.82871828691707539),\n",
       " (u'twitter.com/person/status/1231', 0.3947997711838524),\n",
       " (u'linkedin.com/company/zcbvx', 0.87861670564511407),\n",
       " (u'linkedin.com/in/zcbvx', 0.93873476766805786),\n",
       " (u'en.wikipedia.org/wiki/someone', 0.48006136230111524),\n",
       " (u'en.wikipedia.org/w/index.php?id=123', 0.67484407470849606),\n",
       " (u'www.nytimes.com/topic/person/sheldon-silver', 0.95935597056302369)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uris = [normalize_uri(u) for u in [\n",
    "  'facebook.com/efoim',\n",
    "  'twitter.com/person',\n",
    "  'twitter.com/person/status/1231',\n",
    "  'linkedin.com/company/zcbvx',\n",
    "  'linkedin.com/in/zcbvx',\n",
    "  'en.wikipedia.org/wiki/someone',\n",
    "  'en.wikipedia.org/w/index.php?id=123',\n",
    "  'www.nytimes.com/topic/person/sheldon-silver',\n",
    "]]\n",
    "model.transform(\n",
    "  hashing_tf.transform(\n",
    "    sqlContext.createDataFrame(\n",
    "      [(u, get_uri_features(u)) for u in uris], \n",
    "      ['uri','features'])))\\\n",
    "  .map(lambda r: (r['uri'], r['probability'][1]))\\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc_base_path = 's3n://aws-publicdatasets/'\n",
    "cc_crawl_root = 'common-crawl/crawl-data/CC-MAIN-2016-07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_wats(lines):\n",
    "    def to_wat(record):\n",
    "        if record and len(record) >= 10 and record[9].startswith('{\"Envelope\":{'):\n",
    "            return json.loads('\\n'.join(record[9:]))\n",
    "        return None\n",
    "\n",
    "    record = []\n",
    "    for line in lines:\n",
    "        if line == 'WARC/1.0':\n",
    "            w = to_wat(record)\n",
    "            if w: yield w\n",
    "            record = [line]\n",
    "        else:\n",
    "            record.append(line)\n",
    "    w = to_wat(record)\n",
    "    if w: yield w\n",
    "def extract_links(record):\n",
    "    nil = {}\n",
    "    url = record\\\n",
    "        .get('Envelope', nil)\\\n",
    "        .get('WARC-Header-Metadata', nil)\\\n",
    "        .get('WARC-Target-URI', None)\n",
    "\n",
    "    if url:\n",
    "        links = record\\\n",
    "            .get('Envelope', nil)\\\n",
    "            .get('Payload-Metadata', nil)\\\n",
    "            .get('HTTP-Response-Metadata', nil)\\\n",
    "            .get('HTML-Metadata', nil)\\\n",
    "            .get('Links', [])\n",
    "        for link in links:\n",
    "            if 'text' in link and 'url' in link:\n",
    "                try:\n",
    "                    yield (url, link['text'], urljoin(url, link['url']))\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc_wat_paths = ','.join(sc\\\n",
    "    .textFile(cc_base_path + cc_crawl_root + '/wat.paths.gz')\\\n",
    "    .map(lambda path: cc_base_path + path)\\\n",
    "    .takeSample(False, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anchor_stoplist = sc\\\n",
    "    .textFile(cc_wat_paths)\\\n",
    "    .mapPartitions(parse_wats)\\\n",
    "    .flatMap(extract_links)\\\n",
    "    .filter(lambda (s, a, t): t.startswith('http://') or t.startswith('https://'))\\\n",
    "    .map(lambda (s, a, t): (a.lower(), 1))\\\n",
    "    .reduceByKey(add)\\\n",
    "    .sortBy(lambda (k,v): v, ascending=False)\\\n",
    "    .map(lambda (k, v): k)\\\n",
    "    .take(50)\n",
    "anchor_stoplist = set(anchor_stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3n://aws-publicdatasets/'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc_wat_paths = ','.join(sc\\\n",
    "    .textFile(cc_base_path + cc_crawl_root + '/wat.paths.gz')\\\n",
    "    .map(lambda path: 's3://aws-publicdatasets/' + path)\\\n",
    "    .sample(False, 0.1)\\\n",
    "    .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc_links = sqlContext.createDataFrame(\n",
    "    sc\\\n",
    "        .textFile(cc_wat_paths)\\\n",
    "        .mapPartitions(parse_wats, preservesPartitioning=True)\\\n",
    "        .flatMap(extract_links)\\\n",
    "        .filter(lambda (s, a, t): t.startswith('http://') or t.startswith('https://'))\\\n",
    "        .filter(lambda (s, a, t): a.lower() not in anchor_stoplist)\\\n",
    "        .repartition(4096)\\\n",
    "        .map(lambda (s, a, t): (s, a, t, get_uri_features(normalize_uri(t))))\n",
    "    , ['source', 'anchor', 'target', 'features'])\n",
    "predicted_links = model.transform(hashing_tf.transform(cc_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_links\\\n",
    "    .map(lambda r: (r['source'], r['anchor'], r['target'], r['probability'][1]))\\\n",
    "    .filter(lambda (s,a,t,p): p >= 0.825)\\\n",
    "    .map(json.dumps)\\\n",
    "    .saveAsTextFile(s3baseuri + 'abbrevi8-rnd/web/links/cc0.1/', 'org.apache.hadoop.io.compress.GzipCodec')\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  },
  "name": "Learn Endpoints",
  "notebookId": 48374
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
