{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "if 'VIRTUAL_ENV' in os.environ:\n",
    "    findspark.init(python_path=os.environ['VIRTUAL_ENV']+'/bin/python')\n",
    "else:\n",
    "    findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sqlContext = pyspark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import random\n",
    "import ujson as json\n",
    "from itertools import izip\n",
    "from operator import add, itemgetter\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "import urllib\n",
    "s3baseuri = \"s3a://\"\n",
    "\n",
    "def zip_sum(*x):\n",
    "  return [sum(i) for i in izip(*x)]\n",
    "def trim_link_protocol(s):\n",
    "    idx = s.find('://')\n",
    "    return s if idx == -1 else s[idx+3:]\n",
    "def get_timestamp():\n",
    "  return datetime.fromtimestamp(time()).strftime('%Y%m%d%H%M%S')\n",
    "def write_file_to_s3(localfile, s3_bucket, s3_filename):\n",
    "    from boto.s3.connection import S3Connection\n",
    "    from boto.s3.connection import Key\n",
    "    conn = S3Connection(key, secret)\n",
    "    bucket = conn.get_bucket(s3_bucket)\n",
    "    if len(list(bucket.list(s3_filename))) == 0:\n",
    "        k = Key(bucket)\n",
    "        k.key = s3_filename\n",
    "        k.set_contents_from_filename(localfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_mention_aligned_links(doc):\n",
    "  text = doc['full_text']\n",
    "  for m in doc['mentions']:\n",
    "    mention_start, mention_stop = m['start'], m['stop']\n",
    "    # filter mentions which occur outside of document full_text\n",
    "    if mention_start >= 0 and mention_stop > mention_start:\n",
    "      link_start = mention_stop+2\n",
    "      # naively detect whether this mention sits inside a markdown link anchor\n",
    "      if text[mention_start-1] == '[' and text[mention_stop:link_start] == '](':\n",
    "        link_stop = text.find(')', link_start)\n",
    "\n",
    "        if text[link_start:link_stop].startswith('http://'):\n",
    "            link_start += 7\n",
    "        elif text[link_start:link_stop].startswith('https://'):\n",
    "            link_start += 8\n",
    "\n",
    "        if link_stop != -1:\n",
    "          yield slice(link_start, link_stop), slice(mention_start,mention_stop)\n",
    "\n",
    "def get_links(doc):\n",
    "  for m in re.finditer('(?<!\\\\\\\\)\\[(([^]]|(\\\\\\\\]))+)(]\\(\\s*(http[s]?://)?)([^)]+)\\s*\\)', doc['full_text']):\n",
    "    parts = m.groups()\n",
    "    a, uri = parts[0], parts[5]\n",
    "    if uri and not a.startswith('www') and not a.startswith('http') and not 'secure.adnxs.com' in uri:\n",
    "      if 'digg.com' in uri:\n",
    "        continue # todo: add check for anchor diversity to filter this kidn of thing\n",
    "      mention_start = m.start() + 1\n",
    "      mention_stop = mention_start + len(parts[0])\n",
    "      link_start = mention_stop + len(parts[3])\n",
    "      link_stop = link_start + len(parts[5])\n",
    "      yield slice(link_start, link_stop), slice(mention_start, mention_stop)\n",
    "\n",
    "import base64\n",
    "import urlparse\n",
    "def resolve_hardcoded_redirects(l):\n",
    "  try:\n",
    "    if l.startswith('www.prweb.net'):\n",
    "      l = base64.b64decode(l[len('www.prweb.net/Redirect.aspx?id='):])\n",
    "    elif l.startswith('cts.businesswire.com/ct/') or l.startswith('ctt.marketwire.com/'):\n",
    "      l = urlparse.parse_qs(l)['url'][0]\n",
    "  except: pass\n",
    "  return trim_link_protocol(l)\n",
    "\n",
    "anchor_filters = set([\n",
    "  'facebook',\n",
    "  'twitter',\n",
    "  'zacks investment research',\n",
    "  'reuters',\n",
    "  'linkedin',\n",
    "  'marketbeat'\n",
    "])\n",
    "\n",
    "if False:\n",
    "  def get_link_labels(doc):\n",
    "    text = doc['full_text']\n",
    "    aligned_spans = set()\n",
    "    for l, a in get_mention_aligned_links(doc):\n",
    "      aligned_spans.add((l.start, l.stop))\n",
    "      uri = text[l]\n",
    "      if not 'search' in uri and not text[a].lower().strip() in anchor_filters:\n",
    "        yield (1.0, uri)\n",
    "    for l, a in get_links(doc):\n",
    "      if (l.start, l.stop) not in aligned_spans:\n",
    "        yield (0.0, text[l])\n",
    "\n",
    "def get_anchor_target_pairs(doc):\n",
    "  text = doc['full_text']\n",
    "  aligned_spans = set()\n",
    "  for l, a in get_mention_aligned_links(doc):\n",
    "    aligned_spans.add((l.start, l.stop))\n",
    "    yield to_item(a, l, text, True)\n",
    "  for l, a in get_links(doc):\n",
    "    if (l.start, l.stop) not in aligned_spans:\n",
    "      is_mention = False\n",
    "      if text[a].startswith('@') and not ' ' in text[a]:\n",
    "        is_mention = True # twitter NER = solved\n",
    "      yield to_item(a, l, text, is_mention)\n",
    "\n",
    "def to_item(a, l, text, is_mention, window = 200):\n",
    "    return \\\n",
    "        text[a],\\\n",
    "        resolve_hardcoded_redirects(text[l]),\\\n",
    "        is_mention,\\\n",
    "        text[max(a.start-window, 0):a.start-1],\\\n",
    "        text[l.stop+1:l.stop+window]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc\\\n",
    "    .textFile(raw_corpus_path)\\\n",
    "    .map(json.loads)\\\n",
    "    .flatMap(get_anchor_target_pairs)\\\n",
    "    .take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URI Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_uri(uri):\n",
    "  uri = uri.lower()\n",
    "  if uri.startswith('//'):\n",
    "    uri = uri[2:]\n",
    "  if uri.startswith('www.'):\n",
    "    uri = uri[4:]\n",
    "\n",
    "  # trim uri protocol\n",
    "  idx = uri.find('://')\n",
    "  uri = uri[idx+3:] if idx != -1 else uri\n",
    "\n",
    "  # convert 'blah.com/users.php?id=bob' into 'blah.com/users.php/id=bob'\n",
    "  uri = re.sub('([a-z]+)\\?', r\"\\1/\", uri)\n",
    "  \n",
    "  # convert 'blah.com/users#bob' into 'blah.com/users/bob'\n",
    "  uri = uri.replace('#', '/')\n",
    "\n",
    "  parts = uri.rstrip('/').split('/')\n",
    "  suffix = parts[-1].lower()\n",
    "  if len(parts) > 1 and suffix.startswith('index') or suffix.startswith('default'):\n",
    "    parts = parts[:-1]\n",
    "  if len(parts) > 1:\n",
    "    parts[-1] = '<eid>'\n",
    "  else:\n",
    "    parts.append('<nil>')\n",
    "  return '/'.join(parts)\n",
    "\n",
    "#normalize_uri('vanityfair.com/index.aspx?rofl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_uri_domain(uri):\n",
    "  return uri.split('/')[0]\n",
    "\n",
    "def get_uri_features(uri):\n",
    "  features = []\n",
    "\n",
    "  uri_parts = re.sub('[0-9]', 'N', uri).split('/')\n",
    "  dom = uri_parts[0]\n",
    "  uri_parts[0] = \"<domain>\"\n",
    "  features += list('/'.join(p) for p in izip(uri_parts, uri_parts[1:]))\n",
    "  features += [dom+':'+f for f in features]\n",
    "  features += uri_parts\n",
    "\n",
    "  dom_parts = dom.split('.')\n",
    "  if len(dom_parts) >= 3:\n",
    "    features.append('SD:' + '.'.join(dom_parts[:-2]))\n",
    "  return features\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes, LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, StringIndexer, CountVectorizer\n",
    "\n",
    "def balance_dataset(dataset, minor = 1.0, major = 0.0):\n",
    "  major_count = dataset.filter(dataset.label == major).count()\n",
    "  minor_count = dataset.filter(dataset.label == minor).count()\n",
    "  return dataset.filter(dataset.label == major)\\\n",
    "                .sample(withReplacement=False, fraction=minor_count/float(major_count))\\\n",
    "                .unionAll(dataset.filter(dataset.label == minor))\n",
    "\n",
    "def stats_at_p(r, p):\n",
    "  tp = 1.0 if (r['label'] == 1.0 and r['probability'][1] >= p) else 0.0\n",
    "  fp = 1.0 if (r['label'] == 0.0 and r['probability'][1] >= p) else 0.0\n",
    "  fn = 1.0 if (r['label'] == 1.0 and r['probability'][1] < p) else 0.0\n",
    "  return p, (tp, fp, fn)\n",
    "\n",
    "def evaluate(dataset, ps = None):\n",
    "  if ps == None:\n",
    "    ps = [0.5]\n",
    "  stats_by_p = dataset\\\n",
    "    .flatMap(lambda r: (stats_at_p(r, p) for p in ps))\\\n",
    "    .reduceByKey(lambda a, b: [x+y for x,y in zip(a, b)])\\\n",
    "    .filter(lambda (p, (tp, fp, fn)): (tp+fp) > 0 and (tp+fn) > 0)\\\n",
    "    .mapValues(lambda (tp, fp, fn): ((float(tp) / (tp+fp)), (float(tp) / (tp+fn))))\\\n",
    "    .mapValues(lambda (p, r): (p, r, 2 * (p*r/(p+r))))\\\n",
    "    .collect()\n",
    "  return stats_by_p\n",
    "    \n",
    "classifier = LogisticRegression(featuresCol=\"hashed_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REBUILD_CORPUS = False\n",
    "\n",
    "raw_corpus_path = 'articles'\n",
    "link_corpus_path = 'links'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anchor_target_pairs = sc\\\n",
    "  .textFile(raw_corpus_path)\\\n",
    "  .map(json.loads)\\\n",
    "  .flatMap(get_anchor_target_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anchor_target_pairs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if REBUILD_CORPUS:\n",
    "    train, test = [\n",
    "        split.flatMap(lambda (prefix, instances): instances)\\\n",
    "             .map(lambda (uri, is_mention): (uri, 1.0 if is_mention else 0.0, get_uri_features(uri)))\\\n",
    "             .repartition(128)\\\n",
    "             .cache()\n",
    "        for split in\n",
    "          anchor_target_pairs\\\n",
    "              .map(lambda (anchor, target, is_mention, left, right): (normalize_uri(target), is_mention))\\\n",
    "              .groupByKey()\\\n",
    "              .filter(lambda (k,vs): len(vs) >= 10)\\\n",
    "              .mapValues(Counter)\\\n",
    "              .mapValues(lambda cs: cs[True] > cs[False])\\\n",
    "              .map(lambda (uri, is_mention): (get_uri_domain(uri), (uri, is_mention)))\\\n",
    "              .groupByKey()\\\n",
    "              .randomSplit([0.9, 0.1])\n",
    "    ]\n",
    "    sqlContext\\\n",
    "        .createDataFrame(train, ['uri','label','features'])\\\n",
    "        .write.mode('overwrite')\\\n",
    "        .save(link_corpus_path + '/train')\n",
    "    sqlContext\\\n",
    "        .createDataFrame(test, ['uri','label','features'])\\\n",
    "        .write.mode('overwrite')\\\n",
    "        .save(link_corpus_path + '/test')\n",
    "\n",
    "train = sqlContext.load(link_corpus_path + '/train')\n",
    "test = sqlContext.load(link_corpus_path + '/test')\n",
    "full = train.unionAll(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.filter(train['label']==1.0).count(), train.filter(train['label']==0.0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashing_tf = HashingTF(inputCol=\"features\", outputCol=\"hashed_features\", numFeatures=500000)\n",
    "#hashing_tf = CountVectorizer(inputCol=\"features\", outputCol=\"hashed_features\").fit(train)\n",
    "train = hashing_tf.transform(train)\n",
    "test = hashing_tf.transform(test)\n",
    "dev_model = classifier.fit(balance_dataset(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_labeled_uris(lines):\n",
    "  reader = csv.DictReader(lines, skipinitialspace=True)\n",
    "  for r in reader:\n",
    "    if r['_golden'] == 'true':\n",
    "      continue\n",
    "    yield r['a'], r['is_web_page_a_an_entity_page'] == 'yes', float(r['is_web_page_a_an_entity_page:confidence'])\n",
    "    yield r['b'], r['is_web_page_b_an_entity_page'] == 'yes', float(r['is_web_page_b_an_entity_page:confidence'])\n",
    "\n",
    "labeled_uris = sqlContext.createDataFrame(sc\\\n",
    "  .parallelize(\n",
    "    iter_labeled_uris(sc\\\n",
    "    .textFile(s3baseuri + 'abbrevi8-rnd/web/links/a885525.csv')\\\n",
    "    .map(lambda r: r.encode('utf-8'))\\\n",
    "    .collect()))\\\n",
    "  .map(lambda (uri, entity, conf): (uri, 1.0 if entity else 0.0, conf, get_uri_features(normalize_uri(uri))))\n",
    ", ['uri', 'label', 'confidence', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_prs = evaluate(dev_model.transform(train), ps=[p/20. for p in xrange(1, 20)])\n",
    "dev_prs = evaluate(dev_model.transform(test), ps=[p/20. for p in xrange(1, 20)])\n",
    "test_prs = evaluate(dev_model.transform(hashing_tf.transform(labeled_uris)), ps=[p/20. for p in xrange(1, 20)])\n",
    "\n",
    "print 'Evaluation @ Confidence >= 0.5'\n",
    "print 'Train P/R=(%.2f, %.2f), F=%.3f' % dict(train_prs)[0.8]\n",
    "print '  Dev P/R=(%.2f, %.2f), F=%.3f' % dict(dev_prs)[0.8]\n",
    "print ' Test P/R=(%.2f, %.2f), F=%.3f' % dict(test_prs)[0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display(sorted([(k, v[0]*100, v[1]*100) for k,v in dev_prs]+[(0., 0., 100.)]+[(1., 100.0, 0.)]))\n",
    "display(sorted([(k, v[0], v[1], v[2]) for k,v in dev_prs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_hashing_tf = HashingTF(inputCol=\"features\", outputCol=\"hashed_features\", numFeatures=500000)\n",
    "model = classifier.fit(balance_dataset(full_hashing_tf.transform(full)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uris = [\n",
    "  'facebook.com/efoim',\n",
    "  'twitter.com/person',\n",
    "  'twitter.com/person/status/1231',\n",
    "  'linkedin.com/company/zcbvx',\n",
    "  'linkedin.com/in/zcbvx',\n",
    "  'en.wikipedia.org/wiki/someone',\n",
    "  'en.wikipedia.org/w/index.php?id=123',\n",
    "  'www.nytimes.com/topic/person/sheldon-silver',\n",
    "]\n",
    "model.transform(\n",
    "  full_hashing_tf.transform(\n",
    "    sqlContext.createDataFrame(\n",
    "      [(u, get_uri_features(normalize_uri(u))) for u in uris], \n",
    "      ['uri','features'])))\\\n",
    "  .map(lambda r: (r['uri'], r['probability'][1]))\\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc_target_counts = sc\\\n",
    "  .textFile(s3baseuri + 'abbrevi8-rnd/web/links/cc/')\\\n",
    "  .map(lambda line: line.split('\\t'))\\\n",
    "  .filter(lambda ps: len(ps) == 3)\n",
    "\n",
    "cc_corpus_links = sqlContext.createDataFrame(\n",
    "  sc\\\n",
    "    .textFile(s3baseuri + 'abbrevi8-rnd/web/links/cc/')\\\n",
    "    .sample(False, 0.1)\\\n",
    "    .map(lambda line: line.split('\\t'))\\\n",
    "    .filter(lambda ps: len(ps) == 3)\\\n",
    "    .map(lambda (source, anchor, target): target)\\\n",
    "    .map(trim_link_protocol)\\\n",
    "    .filter(lambda uri: ('.co' in uri or '.net' in uri or '.org' in uri or '.edu' in uri))\\\n",
    "    .map(lambda t: (normalize_uri(t), 1))\\\n",
    "    .reduceByKey(add)\\\n",
    "    .map(lambda (uri, count): (uri, count, get_uri_features(uri)))\n",
    "  ,['uri', 'count', 'features'])\n",
    "\n",
    "corpus_links = model.transform(full_hashing_tf.transform(cc_corpus_links))\n",
    "\n",
    "corpus_links\\\n",
    "  .filter(corpus_links['prediction'] == 1.0)\\\n",
    "  .map(lambda r: (r['uri'], r['count'], r['probability'][1]))\\\n",
    "  .filter(lambda (uri, count, p): p >= 0.8)\\\n",
    "  .map(lambda (uri, count, p): count)\\\n",
    "  .sum()\n",
    "corpus_links\\\n",
    "  .map(lambda r: r['count'])\\\n",
    "  .sum()\n",
    "corpus_links\\\n",
    "  .filter(corpus_links['prediction'] == 1.0)\\\n",
    "  .map(lambda r: (r['uri'], r['count'], r['probability'][1]))\\\n",
    "  .filter(lambda (uri, count, p): p >= 0.85)\\\n",
    "  .map(lambda (u,c,p): (normalize_uri(u), c))\\\n",
    "  .reduceByKey(add)\\\n",
    "  .sortBy(lambda (u,c): c, ascending=False)\\\n",
    "  .take(500)\n",
    "\"\"\"\n",
    "(u'twitter.com/<eid>', 250606),\n",
    " (u'shop.nordstrom.com/c/<eid>', 96401),\n",
    " (u'avo.alaska.edu/activity/avoreport.php/<eid>', 29133),\n",
    " (u'failblog.cheezburger.com/<eid>', 15984),\n",
    " (u'cnbc.com/<eid>', 15504),\n",
    " (u'archive.org/details/<eid>', 13158),\n",
    " (u'lightology.com/index.php/<eid>', 12381),\n",
    " (u'linkedin.com/company/<eid>', 11318),\n",
    " (u'wikia.com/<eid>', 9871),\n",
    " (u'jjill.com/jjillonline/prodnav/grid.aspx/<eid>', 9388),\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_links = full_hashing_tf\\\n",
    "  .transform(\n",
    "    sqlContext.createDataFrame(\n",
    "      anchor_target_pairs\\\n",
    "        .map(lambda (anchor, target, mention): (anchor, target, mention, get_uri_features(target)))\n",
    "      , ['anchor', 'uri','label','features']))\n",
    "corpus_links = model.transform(corpus_links).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_links\\\n",
    "  .rdd\\\n",
    "  .filter(lambda r: 'facebook' in r['uri'] and not r['uri'].endswith('/'))\\\n",
    "  .map(lambda r: (r['uri'], r['anchor'], r['probability']))\\\n",
    "  .take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_pairs = corpus_links\\\n",
    "  .rdd\\\n",
    "  .filter(lambda r: r['probability'][1] >= 0.65)\\\n",
    "  .map(lambda r: (r['anchor'], r['uri']))\\\n",
    "  .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_pairs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uris = set()\n",
    "graph = defaultdict(set)\n",
    "for a, u in predicted_pairs.collect():\n",
    "  a = a.lower()\n",
    "  if '.' in u:\n",
    "    graph[a].add(u)\n",
    "    graph[u].add(a)\n",
    "    uris.add(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_neighbours(graph, node, depth):\n",
    "  pending = set([node])\n",
    "  clique = set(pending)\n",
    "  for _ in xrange(depth):\n",
    "    pending = set().union(*[graph[node] for node in pending])\n",
    "    pending = pending - clique\n",
    "    if not pending:\n",
    "      break\n",
    "    clique = clique | pending\n",
    "  return clique\n",
    "\n",
    "def normalize_uri(u):\n",
    "  u = u.lower().strip('/')\n",
    "  u = urllib.unquote(u)\n",
    "  if u.startswith('www.'):\n",
    "    u = u[4:]\n",
    "  return u\n",
    "\n",
    "uri_nodes = list(uris)\n",
    "neighborhood_sz = 2\n",
    "#sampled_domains = Counter()\n",
    "#sampled_uris = set()\n",
    "#samples = []\n",
    "while len(samples) < 1000:\n",
    "  node = random.choice(uri_nodes)\n",
    "  neighbours = get_neighbours(graph, node, 4) & uris\n",
    "  if len(neighbours) > 1:\n",
    "    neighborhood = random.sample(neighbours, min(neighborhood_sz, len(neighbours)))\n",
    "    b = neighborhood.pop()\n",
    "    while neighborhood:\n",
    "      a, b = b, neighborhood.pop()\n",
    "      norm_a, norm_b = normalize_uri(a), normalize_uri(b)\n",
    "      if norm_a == norm_b or norm_a in sampled_uris or norm_b in sampled_uris:\n",
    "        continue\n",
    "      dom_a, dom_b = norm_a.split('/')[0], norm_b.split('/')[0]\n",
    "      if dom_a == dom_b or dom_a.startswith(dom_b) or dom_b.startswith(dom_a):\n",
    "        continue\n",
    "      dom_samples = max(sampled_domains[dom_a], sampled_domains[dom_b])\n",
    "      if random.random() > math.sqrt(dom_samples/25.0):\n",
    "        samples.append((a,b))\n",
    "        sampled_uris.add(norm_a)\n",
    "        sampled_uris.add(norm_b)\n",
    "        sampled_domains[dom_a] += 1\n",
    "        sampled_domains[dom_b] += 1\n",
    "        #print '%s\\t%s' % (a,b)\n",
    "\n",
    "if True:\n",
    "  with open('/tmp/samples.csv', 'w') as f:\n",
    "    f.write('a,b\\n')\n",
    "    for s in samples:\n",
    "      f.write(('\"'+'\",\"'.join(s)+'\"\\n').encode('utf-8'))\n",
    "  write_file_to_s3('/tmp/samples.csv', 'abbrevi8-rnd', '/web/annotation/samples.%s.csv' % get_timestamp())\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(['\"a\"' in a+b for a,b in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#node = random.sample(graph, 1)[0]\n",
    "#node = 'abcnews.go.com/topics/news/minnesota.htm'\n",
    "node = random.sample(uris, 1)[0]\n",
    "print node\n",
    "#[n for n in get_neighbours(graph, node, 1) if n in uris]\n",
    "get_neighbours(graph, node, 4) & uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  tf_train\\\n",
    "    .filter(tf_train['prediction'] == 1.0)\\\n",
    "    .map(lambda r: (get_uri_prefix(r['uri'])))\\\n",
    "    .reduceByKey(add)\\\n",
    "    .map(lambda (k,v): (v,k))\\\n",
    "    .sortByKey(ascending=False)\\\n",
    "    .filter(lambda (k,v): k <= 500)\\\n",
    "    .take(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs\\\n",
    "  .map(lambda d: (trim_link_protocol(d['source_url']).split('/')[0], set(d['full_text'][ms] for ls, ms in get_mention_aligned_links(d))))\\\n",
    "  .mapValues(lambda anchors: set(a.lower().strip() for a in anchors))\\\n",
    "  .flatMap(lambda (src, anchors): [(a, 1) for a in anchors])\\\n",
    "  .reduceByKey(add)\\\n",
    "  .filter(lambda (k, count): count > 1)\\\n",
    "  .map(lambda (k, count): (count, k))\\\n",
    "  .sortByKey(ascending=False)\\\n",
    "  .take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchors by Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs\\\n",
    "  .map(lambda d: (trim_link_protocol(d['source_url']).split('/')[0], set(d['full_text'][ms] for ls, ms in get_mention_aligned_links(d))))\\\n",
    "  .mapValues(lambda anchors: set(a.lower().strip() for a in anchors))\\\n",
    "  .flatMap(lambda (src, anchors): [((src, a), 1) for a in anchors] + [((src, None), 1)])\\\n",
    "  .reduceByKey(add)\\\n",
    "  .filter(lambda (k, count): count > 1)\\\n",
    "  .map(lambda ((src, a), count): (src, (count, a)))\\\n",
    "  .groupByKey()\\\n",
    "  .mapValues(lambda vs: sorted(vs, reverse=True)[:5])\\\n",
    "  .mapValues(lambda vs: [(a, v/float(vs[0][0]), v) for v, a in vs])\\\n",
    "  .sortBy(lambda (k, vs): vs[0][2], ascending=False)\\\n",
    "  .take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Mention-Aligned Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs\\\n",
    "  .flatMap(lambda d: (d['full_text'][ls] for ls, ms in get_mention_aligned_links(d)))\\\n",
    "  .map(resolve_hardcoded_redirects)\\\n",
    "  .map(lambda uri: (uri, 1))\\\n",
    "  .reduceByKey(add)\\\n",
    "  .sortBy(lambda (k, v): v, ascending=False)\\\n",
    "  .take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Endpoints Prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_prefixes = docs\\\n",
    "  .flatMap(lambda d: (d['full_text'][ls] for ls, ms in get_mention_aligned_links(d)))\\\n",
    "  .map(resolve_hardcoded_redirects)\\\n",
    "  .map(lambda l: '/'.join(l.split('/')[:-1]) or l)\\\n",
    "  .map(lambda uri: (uri, 1))\\\n",
    "  .reduceByKey(add)\\\n",
    "  .sortBy(lambda (k, v): v, ascending=False)\\\n",
    "  .take(100)\n",
    "top_prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for uri, count in top_prefixes:\n",
    "  print '\\url{%s} & %i \\\\\\\\' % (uri, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs = docs\\\n",
    "  .flatMap(lambda d: ((d['full_text'][ms].lower().strip(), d['full_text'][ls]) for ls, ms in get_links(d)))\\\n",
    "  .filter(lambda (a, uri): a not in anchor_filters)\\\n",
    "  .mapValues(resolve_hardcoded_redirects)\\\n",
    "  .distinct()\\\n",
    "  .map(lambda (label, uri): (label, uri, get_uri_features(uri)))\n",
    "\n",
    "pairs = sqlContext.createDataFrame(pairs, ['label','uri', 'features'])\n",
    "pairs = hashing_tf.transform(pairs)\n",
    "pairs = model.transform(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs = docs\\\n",
    "  .flatMap(lambda d: ((d['full_text'][ms].lower().strip(), d['full_text'][ls]) for ls, ms in get_mention_aligned_links(d)))\\\n",
    "  .filter(lambda (a, uri): a not in anchor_filters)\\\n",
    "  .mapValues(resolve_hardcoded_redirects)\\\n",
    "  .distinct()\\\n",
    "  .map(lambda (label, uri): (label, uri, get_link_features(uri)))\\\n",
    "  .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs\\\n",
    "  .map(lambda (label, uri, features): (label, uri))\\\n",
    "  .filter(lambda (label, uri): uri.startswith('www.nytimes.com/topic/person/'))\\\n",
    "  .map(lambda (k,v): (v, k))\\\n",
    "  .groupByKey()\\\n",
    "  .mapValues(Counter)\\\n",
    "  .filter(lambda (uri, labels): sum(labels.itervalues()) > 1)\\\n",
    "  .take(5)\n",
    "  #.map(lambda (k,v): len(v))\\\n",
    "  #.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdf = sqlContext.createDataFrame(pairs, ['label','uri', 'features'])\n",
    "pdf = hashing_tf.transform(pdf)\n",
    "pdf = model.transform(pdf)\n",
    "pdf\\\n",
    "  .rdd\\\n",
    "  .filter(lambda r: r['probability'][1] > 0.5)\\\n",
    "  .map(lambda r: (r['label'],r['uri']))\\\n",
    "  .take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Classified Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  top_classified_endpoints = pdf\\\n",
    "    .rdd\\\n",
    "    .map(lambda r: (r['uri'], r['probability'][1]))\\\n",
    "    .map(lambda (l, p): ('/'.join(l.split('/')[:-1]) or l, p))\\\n",
    "    .map(lambda uri: (uri, 1))\\\n",
    "    .reduceByKey(add)\\\n",
    "    .sortBy(lambda (k, v): v, ascending=False)\\\n",
    "    .take(100)\n",
    "\n",
    "for uri, count in top_classified_endpoints:\n",
    "  print '\\url{%s} & %.2f & %i \\\\\\\\' % (uri[0], uri[1], count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "  most_likely_endpoints = pdf\\\n",
    "    .rdd\\\n",
    "    .map(lambda r: (r['uri'], r['probability'][1]))\\\n",
    "    .map(lambda (l, p): ('/'.join(l.split('/')[:-1]) or l, p))\\\n",
    "    .map(lambda uri: (uri, 1))\\\n",
    "    .reduceByKey(add)\\\n",
    "    .filter(lambda (k,v): v > 200)\\\n",
    "    .sortBy(lambda (k, v): k[1], ascending=False)\\\n",
    "    .take(100)\n",
    "for uri, count in most_likely_endpoints:\n",
    "  print '\\url{%s} & %.2f & %i \\\\\\\\' % (uri[0], uri[1], count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\\url{economictimes.indiatimes.com/topic} & 13553 \\\\\n",
    "\\url{www.huffingtonpost.com/news} & 13483 \\\\\n",
    "\\url{en.wikipedia.org/wiki} & 7241 \\\\\n",
    "\\url{www.huffingtonpost.com.au/news} & 6350 \\\\\n",
    "\\url{data.cnbc.com/quotes} & 3215 \\\\\n",
    "\\url{www.globenewswire.com/newsroom} & 2410 \\\\\n",
    "\\url{www.livemint.com/Search/Link/Keyword} & 2342 \\\\\n",
    "\\url{www.linkedin.com/in} & 2300 \\\\\n",
    "\\url{www.benzinga.com/stock} & 2176 \\\\\n",
    "\\url{sports.yahoo.com/soccer/players} & 2022 \\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdf = sqlContext.createDataFrame(pairs, ['label','uri', 'features'])\n",
    "pdf = hashing_tf.transform(pdf)\n",
    "pdf = model.transform(pdf)\n",
    "pdf = pdf\\\n",
    "  .rdd\\\n",
    "  .filter(lambda r: r['probability'][1] > 0.95)\\\n",
    "  .map(lambda r: (r['label'],r['uri']))\\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.sample(graph, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  },
  "name": "Learn Endpoints",
  "notebookId": 48374
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
